{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACsgGp0AbjCJ"
      },
      "outputs": [],
      "source": [
        "### INSTALL DEPS QUIETLY\n",
        "!pip install -U -q tfds-nightly tf-models-official==2.7.0 \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "from typing_extensions import Literal, ClassVar\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "YMM68iDNe48Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT COMPRESS LOADED/EXPORTED MODEL\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\""
      ],
      "metadata": {
        "id": "mObgcu-ufAcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Split(Enum):\n",
        "  TRAIN = \"train\"\n",
        "  VALIDATION = \"validation\"\n",
        "  TEST = \"test\"\n",
        "\n",
        "class Languages(Enum):\n",
        "  ru = 1\n",
        "  en = 0"
      ],
      "metadata": {
        "id": "zsTNAsPHiIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### UTILS\n",
        "def convert_dataframe_column_to_tensor(seq: pd.Series, dtype, name:str) -> tf.Tensor:\n",
        "  return tf.convert_to_tensor(seq, dtype=dtype, name=name)\n"
      ],
      "metadata": {
        "id": "wEwJeM_dhDkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageDetection:\n",
        "  tensorflow_preprocess_handle: ClassVar[str] = 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3'\n",
        "  tensorflow_model: ClassVar[str] = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'\n",
        "  AUTOTUNE: ClassVar[int] = tf.data.AUTOTUNE\n",
        "  batch_size: ClassVar[int] = 32\n",
        "  classes: ClassVar[List[str]] = [\"en\", \"ru\"]\n",
        "  model_path: ClassVar[str] = os.path.join(\".\", \"language-detection\")\n",
        "  features: ClassVar[List[str]] = [\"Text\"]\n",
        "  label_name: ClassVar[str] = \"Language\"\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes: int):\n",
        "      super(LanguageDetection.Classifier, self).__init__(name=\"language_classifier\")\n",
        "      self.encoder = hub.KerasLayer(LanguageDetection.tensorflow_model, trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "      print(f\"Classifier.__call__() called.\")\n",
        "      return super().__call__(*args, **kwargs)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      print(f'Classifier.call({preprocessed_text}) = {x}')\n",
        "      return x\n",
        "\n",
        "  def __init__(self, seq_length=128):\n",
        "    if os.environ.get('COLAB_TPU_ADDR', None):\n",
        "      cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "      tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "      tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "      self.strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "    elif tf.config.list_physical_devices('GPU'):\n",
        "      self.strategy = tf.distribute.MirroredStrategy()\n",
        "    else:\n",
        "      self.strategy = tf.distribute.OneDeviceStrategy(\"/device:CPU:0\")\n",
        "\n",
        "    self.__preprocess_model = self.__make_preprocess_model(self.features, seq_length=seq_length)\n",
        "    self.__reloaded_model = None\n",
        "\n",
        "  def load_dataset(self, dataframes: Dict[Split, pd.DataFrame], split: Split) -> Tuple[tf.data.Dataset, int]:\n",
        "    df = dataframes[split]\n",
        "    data_count = len(df)\n",
        "\n",
        "    tensor_slice: Dict[str, tf.Tensor] = {\n",
        "        self.label_name: convert_dataframe_column_to_tensor(df[self.label_name], dtype=tf.int32, name=f\"{split}-{self.label_name}\")\n",
        "    }\n",
        "    for feature in self.features:\n",
        "      tensor_slice[feature] = convert_dataframe_column_to_tensor(df[feature], dtype=tf.string, name=f\"{split}-{feature}\")\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tensor_slice)\n",
        "    if split == Split.TRAIN:\n",
        "      dataset = dataset.shuffle(data_count)\n",
        "      dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(LanguageDetection.batch_size)\n",
        "    dataset = dataset.map(lambda ex: (self.__preprocess_model(ex), ex[self.label_name]))\n",
        "    dataset = dataset.cache().prefetch(buffer_size=LanguageDetection.AUTOTUNE)\n",
        "    return dataset, data_count\n",
        "\n",
        "  def __make_preprocess_model(self, features: List[str], seq_length = 128):\n",
        "    text_inputs: List[tf.keras.layers.Input] = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in self.features\n",
        "    ]\n",
        "\n",
        "    preprocessor = hub.load(LanguageDetection.tensorflow_preprocess_handle)\n",
        "    tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
        "    tokenized_inputs = [tokenize(s) for s in text_inputs]\n",
        "\n",
        "    bert_pack_inputs = hub.KerasLayer(\n",
        "      preprocessor.bert_pack_inputs,\n",
        "      arguments=dict(seq_length=seq_length),\n",
        "      name='bert_pack_inputs'\n",
        "    )\n",
        "    model_inputs = bert_pack_inputs(tokenized_inputs)\n",
        "    return tf.keras.Model(text_inputs, model_inputs)\n",
        "\n",
        "  def __make_classifier_model(self):\n",
        "    return LanguageDetection.Classifier(len(LanguageDetection.classes))\n",
        "\n",
        "  def fit(self, dataframes: Dict[Split, pd.DataFrame], epochs=3, init_lr=2e-5):\n",
        "    with self.strategy.scope():\n",
        "      metrics = [tfa.metrics.MatthewsCorrelationCoefficient(num_classes=len(LanguageDetection.classes))]\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "      train_dataset, train_datacount = language_detection_model.load_dataset(dataframes=dataframes, split=Split.TRAIN)\n",
        "      validation_dataset, validation_datacount = language_detection_model.load_dataset(dataframes=dataframes, split=Split.VALIDATION)\n",
        "\n",
        "      steps_per_epoch = train_datacount // LanguageDetection.batch_size\n",
        "      num_train_steps = steps_per_epoch * epochs\n",
        "      num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "      validation_steps = validation_datacount // LanguageDetection.batch_size\n",
        "\n",
        "      optimizer = optimization.create_optimizer(\n",
        "          init_lr=init_lr,\n",
        "          num_train_steps=num_train_steps,\n",
        "          num_warmup_steps=num_warmup_steps,\n",
        "          optimizer_type='adamw'\n",
        "      )\n",
        "      self.__model = self.__make_classifier_model()\n",
        "      self.__model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "      self.__model.fit(\n",
        "          x=train_dataset,\n",
        "          validation_data=validation_dataset,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=epochs,\n",
        "          validation_steps=validation_steps\n",
        "      )\n",
        "\n",
        "  def export(self) -> None:\n",
        "    bert_outputs = self.__model(self.__preprocess_model(self.__preprocess_model.inputs))\n",
        "    exported_model = tf.keras.Model(self.__preprocess_model.inputs, bert_outputs)\n",
        "\n",
        "    save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "    exported_model.save(LanguageDetection.model_path, include_optimizer=False,\n",
        "                      options=save_options)\n",
        "    \n",
        "  @classmethod\n",
        "  def evaluate(cls, sentence: List[str]) -> List[str]:\n",
        "    with tf.device('/job:localhost'):\n",
        "      reloaded_model = tf.saved_model.load(cls.model_path)\n",
        "      test_dataset = tf.data.Dataset.from_tensor_slices({\n",
        "          \"Text\": sentence\n",
        "      })\n",
        "\n",
        "      results: List[str] = []\n",
        "\n",
        "      for features in test_dataset.map(lambda rec: [[rec[ft]] for ft in cls.features]):\n",
        "        if len(cls.features) == 1:\n",
        "          result = reloaded_model(features[0])\n",
        "        else:\n",
        "          result = reloaded_model(list(features))\n",
        "        classification = tf.argmax(result, axis=1)[0].numpy().item()\n",
        "        results.append(Languages(classification).name)\n",
        "      \n",
        "      return results\n"
      ],
      "metadata": {
        "id": "wwgmr1XAfcNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LOAD DATASET FOR LANGUAGE DETECTION\n",
        "cleaned_ld_training_data = pd.read_csv(\"cleaned_train.csv\")\n",
        "cleaned_ld_training_data.loc[cleaned_ld_training_data[\"Language\"] == \"ru\", \"Language\"] = 1\n",
        "cleaned_ld_training_data.loc[cleaned_ld_training_data[\"Language\"] == \"en\", \"Language\"] = 0\n",
        "cleaned_ld_training_data[\"Language\"] = cleaned_ld_training_data[\"Language\"].astype('int')\n",
        "cleaned_ld_training_data['Text'] = cleaned_ld_training_data['Text'].astype(\"str\")\n",
        "\n",
        "cleaned_ld_test_data = pd.read_csv(\"cleaned_test.csv\")\n",
        "cleaned_ld_test_data.loc[cleaned_ld_test_data[\"Language\"] == \"ru\", \"Language\"] = 1\n",
        "cleaned_ld_test_data.loc[cleaned_ld_test_data[\"Language\"] == \"en\", \"Language\"] = 0\n",
        "cleaned_ld_test_data[\"Language\"] = cleaned_ld_test_data[\"Language\"].astype('int')\n",
        "cleaned_ld_test_data['Text'] = cleaned_ld_test_data['Text'].astype(\"str\")\n",
        "\n",
        "cleaned_ld_validation_data = pd.read_csv(\"cleaned_validation.csv\")\n",
        "cleaned_ld_validation_data.loc[cleaned_ld_validation_data[\"Language\"] == \"ru\", \"Language\"] = 1\n",
        "cleaned_ld_validation_data.loc[cleaned_ld_validation_data[\"Language\"] == \"en\", \"Language\"] = 0\n",
        "cleaned_ld_validation_data[\"Language\"] = cleaned_ld_validation_data[\"Language\"].astype('int')\n",
        "cleaned_ld_validation_data['Text'] = cleaned_ld_validation_data['Text'].astype(\"str\")\n",
        "\n",
        "language_detection_dataset = {\n",
        "    Split.TRAIN: cleaned_ld_training_data,\n",
        "    Split.TEST: cleaned_ld_test_data,\n",
        "    Split.VALIDATION: cleaned_ld_validation_data\n",
        "}"
      ],
      "metadata": {
        "id": "s4trgckjNq8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_detection_model = LanguageDetection()"
      ],
      "metadata": {
        "id": "jRk3yGeYgBcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_detection_model.fit(dataframes=language_detection_dataset)\n",
        "language_detection_model.export()\n"
      ],
      "metadata": {
        "id": "kTRCO5B9ojAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ld_inputs = language_detection_dataset[Split.TEST]['Text'].to_numpy().tolist()[:5]\n",
        "LanguageDetection.evaluate(ld_inputs)\n"
      ],
      "metadata": {
        "id": "qXHjNQs4YGQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}