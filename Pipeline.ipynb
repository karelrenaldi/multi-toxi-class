{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACsgGp0AbjCJ"
      },
      "outputs": [],
      "source": [
        "### INSTALL DEPS QUIETLY\n",
        "!pip install -U -q tfds-nightly tf-models-official==2.7.0 \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from typing_extensions import Literal, ClassVar\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "YMM68iDNe48Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT COMPRESS LOADED/EXPORTED MODEL\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\""
      ],
      "metadata": {
        "id": "mObgcu-ufAcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Split(Enum):\n",
        "  TRAIN = \"train\"\n",
        "  VALIDATION = \"validation\"\n",
        "  TEST = \"test\"\n"
      ],
      "metadata": {
        "id": "zsTNAsPHiIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### UTILS\n",
        "\n",
        "def convert_dataframe_to_tensor(seq: pd.Series, dtype, name:str) -> tf.Tensor:\n",
        "  return tf.convert_to_tensor(seq, dtype=dtype, name=name)"
      ],
      "metadata": {
        "id": "wEwJeM_dhDkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageDetection:\n",
        "  tensorflow_preprocess_handle: ClassVar[str] = 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3'\n",
        "  tensorflow_model: ClassVar[str] = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'\n",
        "  AUTOTUNE: ClassVar[int] = tf.data.AUTOTUNE\n",
        "  classes: ClassVar[List[str]] = [\"English\", \"Malayalam\", \"Hindi\", \"Tamil\", \"Kannada\", \"French\", \"Spanish\", \"Portuguese\", \"Italian\", \"Russian\", \"Sweedish\", \"Dutch\", \"Arabic\", \"Turkish\", \"German\", \"Danish\", \"Greek\"]\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes: int):\n",
        "      super(LanguageDetection.Classifier, self).__init__(name=\"language_classifier\")\n",
        "      self.encoder = hub.KerasLayer(LanguageDetection.tensorflow_model, trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      return x\n",
        "\n",
        "  def __init__(self, seq_length=128):\n",
        "    if os.environ.get('COLAB_TPU_ADDR', None):\n",
        "      cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "      tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "      tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "      self.strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "    elif tf.config.list_physical_devices('GPU'):\n",
        "      self.strategy = tf.distribute.MirroredStrategy()\n",
        "    else:\n",
        "      self.strategy = tf.distribute.OneDeviceStrategy(\"/device:CPU:0\")\n",
        "\n",
        "    self.features: List[str] = [\"Text\"]\n",
        "    self.label_name: str = \"Language\"\n",
        "\n",
        "    self.__preprocess_model = self.__make_preprocess_model(self.features, seq_length=seq_length)\n",
        "    self.__model = self.__make_classifier_model()\n",
        "\n",
        "  def __load_dataset(self, dataframes: Dict[Split, pd.DataFrame], split: Split, batch_size = 128):\n",
        "    df = dataframes[split]\n",
        "    data_count = len(df)\n",
        "\n",
        "    tensor_slice: Dict[str, tf.Tensor] = {\n",
        "        self.label_name: convert_dataframe_to_tensor(df[self.label_name], dtype=tf.int32, name=self.label_name)\n",
        "    }\n",
        "    for feature in self.features:\n",
        "      tensor_slice[feature] = convert_dataframe_to_tensor(df[feature], dtype=tf.string, name=feature)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tensor_slice)\n",
        "    if split == Split.TRAIN:\n",
        "      dataset = dataset.shuffle(data_count)\n",
        "      dataset = dataset.repeat()\n",
        "    if batch_size > 0:\n",
        "      dataset = dataset.batch(batch_size)\n",
        "    else:\n",
        "      dataset = dataset.batch(data_count)\n",
        "    dataset = dataset.map(lambda ex: (self.__preprocess_model(ex), ex[self.label]))\n",
        "    dataset = dataset.cache().prefetch(buffer_size=LanguageDetection.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "  def __make_preprocess_model(self, features: List[str], seq_length = 128):\n",
        "    \"\"\"\n",
        "    Returns Model mapping string features to BERT inputs.\n",
        "  \n",
        "    See: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3#:~:text=seq_length%3D128.-,General%20usage,-For%20pairs%20of\n",
        "\n",
        "    Args:\n",
        "      sentence_features: a list with the names of string-valued features.\n",
        "      seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "    Returns:\n",
        "      A Keras Model that can be called on a list or dict of string Tensors\n",
        "      (with the order or names, resp., given by sentence_features) and\n",
        "      returns a dict of tensors for input to BERT.\n",
        "    \"\"\"\n",
        "    text_inputs: List[tf.keras.layers.Input] = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in self.features\n",
        "    ]\n",
        "\n",
        "    # Tokenize the text to word pieces.\n",
        "    preprocessor = hub.load(LanguageDetection.tensorflow_preprocess_handle)\n",
        "    tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
        "    # tokenize() returns an int32 RaggedTensor of shape [batch_size, (words), (tokens_per_word)].\n",
        "    tokenized_inputs = [tokenize(s) for s in text_inputs]\n",
        "\n",
        "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "    # are model-dependent, so this gets loaded from the SavedModel.\n",
        "    bert_pack_inputs = hub.KerasLayer(\n",
        "      preprocessor.bert_pack_inputs,\n",
        "      arguments=dict(seq_length=seq_length),\n",
        "      name='bert_pack_inputs'\n",
        "    )\n",
        "    model_inputs = bert_pack_inputs(tokenized_inputs)\n",
        "    return tf.keras.Model(text_inputs, model_inputs)\n",
        "\n",
        "  def __make_classifier_model(self):\n",
        "    return LanguageDetection.Classifier(len(LanguageDetection.classes))\n",
        "\n",
        "  def fit(self, training_data: tf.data.Dataset, validation_data: tf.data.Dataset):\n",
        "    epochs = 3\n",
        "    batch_size = 32\n",
        "    init_lr = 2e-5\n",
        "\n",
        "    ### TODO: Ganti metrics atau sisain class \"English\" ama \"Russian\"\n",
        "    ### MathewsCorrelationCoefficient metrics paling bagus buat 2 class\n",
        "    metrics = [tfa.metrics.MatthewsCorrelationCoefficient(num_classes=len(LanguageDetection.classes))]\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    with self.strategy.scope():\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "wwgmr1XAfcNL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LanguageDetection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRk3yGeYgBcj",
        "outputId": "2b9227b6-8c86-460f-b2d1-b64cc4b03f3a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.LanguageDetection at 0x7f363c7c8ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}