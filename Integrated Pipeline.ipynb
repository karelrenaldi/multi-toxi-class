{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "BfgT1FMGU2aP",
        "klqrMxH9VGVy",
        "IGEg1tMIU7uH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACsgGp0AbjCJ"
      },
      "outputs": [],
      "source": [
        "### INSTALL DEPS QUIETLY\n",
        "!pip install -U -q tensorflow keras datasets tensorflow-estimator tfds-nightly tf-models-official==2.7.0 \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "import datasets as huggingface\n",
        "import tensorflow_text as text\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List, Dict, Tuple\n",
        "from typing_extensions import Literal, ClassVar\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "YMM68iDNe48Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LANGUAGE DETECTION"
      ],
      "metadata": {
        "id": "CkcAfNyBIcXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DO NOT COMPRESS LOADED/EXPORTED MODEL\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\""
      ],
      "metadata": {
        "id": "mObgcu-ufAcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Split(Enum):\n",
        "  TRAIN = \"train\"\n",
        "  VALIDATION = \"validation\"\n",
        "  TEST = \"test\"\n",
        "\n",
        "class Languages(Enum):\n",
        "  ru = 1\n",
        "  en = 0"
      ],
      "metadata": {
        "id": "zsTNAsPHiIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### UTILS\n",
        "def convert_dataframe_column_to_tensor(seq: pd.Series, dtype, name:str) -> tf.Tensor:\n",
        "  return tf.convert_to_tensor(seq, dtype=dtype, name=name)\n"
      ],
      "metadata": {
        "id": "wEwJeM_dhDkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL"
      ],
      "metadata": {
        "id": "fT7jaY0-0wYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageDetection:\n",
        "  tensorflow_preprocess_handle: ClassVar[str] = 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3'\n",
        "  tensorflow_model: ClassVar[str] = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'\n",
        "  AUTOTUNE: ClassVar[int] = tf.data.AUTOTUNE\n",
        "  classes: ClassVar[List[str]] = [\"en\", \"ru\"]\n",
        "  model_path: ClassVar[str] = os.path.join(\".\", \"language-detection\")\n",
        "  features: ClassVar[List[str]] = [\"Text\"]\n",
        "  label_name: ClassVar[str] = \"Language\"\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes: int):\n",
        "      super(LanguageDetection.Classifier, self).__init__(name=\"language_classifier\")\n",
        "      self.encoder = hub.KerasLayer(LanguageDetection.tensorflow_model, trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      return x\n",
        "\n",
        "  def __init__(self, seq_length=128):\n",
        "    if os.environ.get('COLAB_TPU_ADDR', None):\n",
        "      cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "      tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "      tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "      self.strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "    elif tf.config.list_physical_devices('GPU'):\n",
        "      self.strategy = tf.distribute.MirroredStrategy()\n",
        "    else:\n",
        "      # NOT RECOMMENDED.\n",
        "      self.strategy = tf.distribute.OneDeviceStrategy(\"/device:CPU:0\")\n",
        "\n",
        "    self.__preprocess_model = self.__make_preprocess_model(self.features, seq_length=seq_length)\n",
        "    self.__reloaded_model = None\n",
        "\n",
        "  def load_dataset(self, dataframes: Dict[Split, pd.DataFrame], split: Split, batch_size=32) -> Tuple[tf.data.Dataset, int]:\n",
        "    df = dataframes[split]\n",
        "    data_count = len(df)\n",
        "\n",
        "    tensor_slice: Dict[str, tf.Tensor] = {\n",
        "        self.label_name: convert_dataframe_column_to_tensor(df[self.label_name], dtype=tf.int32, name=f\"{split}-{self.label_name}\")\n",
        "    }\n",
        "    for feature in self.features:\n",
        "      tensor_slice[feature] = convert_dataframe_column_to_tensor(df[feature], dtype=tf.string, name=f\"{split}-{feature}\")\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tensor_slice)\n",
        "    if split == Split.TRAIN:\n",
        "      dataset = dataset.shuffle(data_count)\n",
        "      dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda ex: (self.__preprocess_model(ex), ex[self.label_name]))\n",
        "    dataset = dataset.cache().prefetch(buffer_size=LanguageDetection.AUTOTUNE)\n",
        "    return dataset, data_count\n",
        "\n",
        "  def __make_preprocess_model(self, features: List[str], seq_length=128):\n",
        "    text_inputs: List[tf.keras.layers.Input] = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in self.features\n",
        "    ]\n",
        "\n",
        "    preprocessor = hub.load(LanguageDetection.tensorflow_preprocess_handle)\n",
        "    tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
        "    tokenized_inputs = [tokenize(s) for s in text_inputs]\n",
        "\n",
        "    bert_pack_inputs = hub.KerasLayer(\n",
        "      preprocessor.bert_pack_inputs,\n",
        "      arguments=dict(seq_length=seq_length),\n",
        "      name='bert_pack_inputs'\n",
        "    )\n",
        "    model_inputs = bert_pack_inputs(tokenized_inputs)\n",
        "    return tf.keras.Model(text_inputs, model_inputs)\n",
        "\n",
        "  def __make_classifier_model(self):\n",
        "    return LanguageDetection.Classifier(len(LanguageDetection.classes))\n",
        "\n",
        "  def fit(self, dataframes: Dict[Split, pd.DataFrame], epochs=3, init_lr=2e-5, batch_size=32):\n",
        "    with self.strategy.scope():\n",
        "      metrics = [tfa.metrics.MatthewsCorrelationCoefficient(num_classes=len(LanguageDetection.classes))]\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "      train_dataset, train_datacount = self.load_dataset(dataframes=dataframes, split=Split.TRAIN, batch_size=batch_size)\n",
        "      validation_dataset, validation_datacount = self.load_dataset(dataframes=dataframes, split=Split.VALIDATION, batch_size=batch_size)\n",
        "\n",
        "      steps_per_epoch = train_datacount // batch_size\n",
        "      num_train_steps = steps_per_epoch * epochs\n",
        "      num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "      validation_steps = validation_datacount // batch_size\n",
        "\n",
        "      optimizer = optimization.create_optimizer(\n",
        "          init_lr=init_lr,\n",
        "          num_train_steps=num_train_steps,\n",
        "          num_warmup_steps=num_warmup_steps,\n",
        "          optimizer_type='adamw'\n",
        "      )\n",
        "      self.__model = self.__make_classifier_model()\n",
        "      self.__model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "      self.__model.fit(\n",
        "          x=train_dataset,\n",
        "          validation_data=validation_dataset,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=epochs,\n",
        "          validation_steps=validation_steps\n",
        "      )\n",
        "\n",
        "  def export(self) -> None:\n",
        "    bert_outputs = self.__model(self.__preprocess_model(self.__preprocess_model.inputs))\n",
        "    exported_model = tf.keras.Model(self.__preprocess_model.inputs, bert_outputs)\n",
        "\n",
        "    save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "    exported_model.save(LanguageDetection.model_path, include_optimizer=False,\n",
        "                      options=save_options)\n",
        "    \n",
        "  @classmethod\n",
        "  def evaluate(cls, sentence: List[str]) -> List[str]:\n",
        "    with tf.device('/job:localhost'):\n",
        "      reloaded_model = tf.saved_model.load(cls.model_path)\n",
        "      test_dataset = tf.data.Dataset.from_tensor_slices({\n",
        "          \"Text\": sentence\n",
        "      })\n",
        "\n",
        "      results: List[str] = []\n",
        "\n",
        "      for features in test_dataset.map(lambda rec: [[rec[ft]] for ft in cls.features]):\n",
        "        if len(cls.features) == 1:\n",
        "          result = reloaded_model(features[0])\n",
        "        else:\n",
        "          result = reloaded_model(list(features))\n",
        "        classification = tf.argmax(result, axis=1)[0].numpy().item()\n",
        "        results.append(Languages(classification).name)\n",
        "      \n",
        "      return pd.DataFrame.from_dict(data=dict(Text=sentence, Language=results))\n"
      ],
      "metadata": {
        "id": "wwgmr1XAfcNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD DATASET"
      ],
      "metadata": {
        "id": "nxAZ_fBn0qKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str):\n",
        "    text =  text.lower()\n",
        "    # Change abbreviation\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"\\r\", \"\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    # Remove special characters/punctuation \n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
        "    text = re.sub(\"(\\\\W)\",\" \",text) \n",
        "    # Remove spaces and digits\n",
        "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
        "    \n",
        "    return text\n"
      ],
      "metadata": {
        "id": "f_DB5YRw5a2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True, as_supervised=True)\n",
        "huggingface_dataset: huggingface.DatasetDict = huggingface.load_dataset(\"papluca/language-identification\")\n",
        "\n",
        "try:\n",
        "  with open('cleaned_train.csv', 'r'):\n",
        "    pass\n",
        "except:\n",
        "  df = tfds.as_dataframe(examples['train'], metadata)\n",
        "  from_tfds = pd.concat([\n",
        "    pd.DataFrame(dict(Text=df[\"en\"], Language=\"en\"))[::2], \n",
        "    pd.DataFrame(dict(Text=df[\"ru\"], Language=\"ru\"))[::2]\n",
        "  ])\n",
        "  from_tfds[\"Text\"] = from_tfds[\"Text\"].apply(lambda x: clean_text(x.decode(\"utf-8\")))\n",
        "\n",
        "  columns = [\"Text\", \"Language\"]\n",
        "\n",
        "  ### HUGGINGFACE_DATASET_URL = https://huggingface.co/datasets/papluca/language-identification\n",
        "  df2 = huggingface_dataset[\"train\"].to_pandas()\n",
        "  df2 = df2.rename(columns={ \"labels\": \"Language\", \"text\": \"Text\" })\n",
        "  df2 = df2[(df2['Language'] == \"en\") | (df2[\"Language\"] == \"ru\")]\n",
        "  df2 = df2.reindex(columns=columns)\n",
        "  # df2.to_csv('./cleaned_train.csv', columns=columns, index=False)\n",
        "  pd.concat([df2, from_tfds]).to_csv(\"./cleaned_train.csv\", columns=columns, index=False)\n",
        "\n",
        "\n",
        "try:\n",
        "  with open('cleaned_test.csv', 'r'):\n",
        "    pass\n",
        "except:\n",
        "  df = tfds.as_dataframe(examples['test'], metadata)\n",
        "  from_tfds = pd.concat([\n",
        "    pd.DataFrame(dict(Text=df[\"en\"], Language=\"en\"))[::2], \n",
        "    pd.DataFrame(dict(Text=df[\"ru\"], Language=\"ru\"))[::2]\n",
        "  ])\n",
        "  from_tfds[\"Text\"] = from_tfds[\"Text\"].apply(lambda x: clean_text(x.decode(\"utf-8\")))\n",
        "  \n",
        "  df3 = huggingface_dataset[\"test\"].to_pandas()\n",
        "  df3 = df3.rename(columns={ \"labels\": \"Language\", \"text\": \"Text\" })\n",
        "  df3 = df3[(df3['Language'] == \"en\") | (df3[\"Language\"] == \"ru\")]\n",
        "  df3 = df3.reindex(columns=columns)\n",
        "  # df3.to_csv('./cleaned_test.csv', columns=columns, index=False)\n",
        "  pd.concat([df3, from_tfds]).to_csv(\"./cleaned_test.csv\", columns=columns, index=False)\n",
        "\n",
        "try:\n",
        "  with open('cleaned_validation.csv', 'r'):\n",
        "    pass\n",
        "except:\n",
        "  df = tfds.as_dataframe(examples[\"validation\"], metadata)\n",
        "  from_tfds = pd.concat([\n",
        "    pd.DataFrame(dict(Text=df[\"en\"], Language=\"en\"))[::2], \n",
        "    pd.DataFrame(dict(Text=df[\"ru\"], Language=\"ru\"))[::2]\n",
        "  ])\n",
        "  from_tfds[\"Text\"] = from_tfds[\"Text\"].apply(lambda x: clean_text(x.decode(\"utf-8\")))\n",
        "  \n",
        "  df4 = huggingface_dataset[\"validation\"].to_pandas()\n",
        "  df4 = df4.rename(columns={ \"labels\": \"Language\", \"text\": \"Text\" })\n",
        "  df4 = df4[(df4['Language'] == \"en\") | (df4[\"Language\"] == \"ru\")]\n",
        "  df4 = df4.reindex(columns=columns)\n",
        "  # df4.to_csv('./cleaned_validation.csv', columns=columns, index=False)\n",
        "  pd.concat([df4, from_tfds]).to_csv(\"./cleaned_validation.csv\", columns=columns, index=False)"
      ],
      "metadata": {
        "id": "PE5ocvZo1TV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LOAD DATASET FOR LANGUAGE DETECTION\n",
        "cleaned_ld_training_data = pd.read_csv(\"cleaned_train.csv\")\n",
        "cleaned_ld_training_data.loc[cleaned_ld_training_data[\"Language\"] == \"ru\", \"Language\"] = 1\n",
        "cleaned_ld_training_data.loc[cleaned_ld_training_data[\"Language\"] == \"en\", \"Language\"] = 0\n",
        "cleaned_ld_training_data[\"Language\"] = cleaned_ld_training_data[\"Language\"].astype('int')\n",
        "cleaned_ld_training_data['Text'] = cleaned_ld_training_data['Text'].astype(\"str\")\n",
        "\n",
        "cleaned_ld_test_data = pd.read_csv(\"cleaned_test.csv\")\n",
        "cleaned_ld_test_data.loc[cleaned_ld_test_data[\"Language\"] == \"ru\", \"Language\"] = 1\n",
        "cleaned_ld_test_data.loc[cleaned_ld_test_data[\"Language\"] == \"en\", \"Language\"] = 0\n",
        "cleaned_ld_test_data[\"Language\"] = cleaned_ld_test_data[\"Language\"].astype('int')\n",
        "cleaned_ld_test_data['Text'] = cleaned_ld_test_data['Text'].astype(\"str\")\n",
        "\n",
        "cleaned_ld_validation_data = pd.read_csv(\"cleaned_validation.csv\")\n",
        "cleaned_ld_validation_data.loc[cleaned_ld_validation_data[\"Language\"] == \"ru\", \"Language\"] = 1\n",
        "cleaned_ld_validation_data.loc[cleaned_ld_validation_data[\"Language\"] == \"en\", \"Language\"] = 0\n",
        "cleaned_ld_validation_data[\"Language\"] = cleaned_ld_validation_data[\"Language\"].astype('int')\n",
        "cleaned_ld_validation_data['Text'] = cleaned_ld_validation_data['Text'].astype(\"str\")\n",
        "\n",
        "language_detection_dataset = {\n",
        "    Split.TRAIN: cleaned_ld_training_data,\n",
        "    Split.TEST: cleaned_ld_test_data,\n",
        "    Split.VALIDATION: cleaned_ld_validation_data\n",
        "}"
      ],
      "metadata": {
        "id": "s4trgckjNq8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING"
      ],
      "metadata": {
        "id": "Cb7k99kv0sRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_detection_model = LanguageDetection()"
      ],
      "metadata": {
        "id": "jRk3yGeYgBcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_detection_model.fit(dataframes=language_detection_dataset, epochs=3, batch_size=16)\n",
        "language_detection_model.export()\n"
      ],
      "metadata": {
        "id": "kTRCO5B9ojAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVALUATION"
      ],
      "metadata": {
        "id": "QryPfEBmDrn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_as_dict = dict(language_detection_dataset[Split.TEST])\n",
        "ld_texts, ld_labels = test_df_as_dict['Text'].to_numpy().tolist(), test_df_as_dict['Language'].to_numpy().tolist()\n",
        "\n",
        "sentences_language = LanguageDetection.evaluate(ld_texts)\n",
        "# russian = 1\n",
        "# english = 0\n",
        "true_positive = 0\n",
        "true_negative = 0\n",
        "false_positive = 0\n",
        "false_negative = 0\n",
        "for i in range(len(language_detection_dataset[Split.TEST])):\n",
        "  if ld_labels[i] == 1:\n",
        "    if sentences_language.iloc[i]['Language'] == Languages(ld_labels[i]).name:\n",
        "      true_positive += 1\n",
        "    else:\n",
        "      false_negative += 1\n",
        "  else:\n",
        "    if sentences_language.iloc[i]['Language'] == Languages(ld_labels[i]).name:\n",
        "      true_negative += 1\n",
        "    else:\n",
        "      false_positive += 1\n",
        "  \n",
        "print(true_positive, false_positive)\n",
        "print(false_negative, true_negative)"
      ],
      "metadata": {
        "id": "rZrX2iuADtt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MACHINE TRANSLATION"
      ],
      "metadata": {
        "id": "JkFxPc2CIg51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD DATASET"
      ],
      "metadata": {
        "id": "jDng19tWMRPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "id": "YDdnKuAgMKMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOKENIZATION"
      ],
      "metadata": {
        "id": "rSQIMY4kMSfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en = train_examples.map(lambda ru, en: en)\n",
        "train_ru = train_examples.map(lambda ru, en: ru)"
      ],
      "metadata": {
        "id": "oe-UP5i2MVuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size=8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=dict(lower_case=True),\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "id": "NQp8WlNYMdRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS PART WILL AUTOMATICALLY CHECK YOUR VOCAB\n",
        "VOCAB_FILE_DIR = os.path.join(os.getcwd(), 'machine-translation-vocab')\n",
        "\n",
        "def write_vocab_file(filename: str, vocab: List[str]):\n",
        "  os.makedirs(VOCAB_FILE_DIR, exist_ok=True)\n",
        "  with open(os.path.join(VOCAB_FILE_DIR, filename), 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)\n",
        "\n",
        "try:\n",
        "  with open(os.path.join(VOCAB_FILE_DIR, 'en_vocab.txt'), 'r'):\n",
        "    pass\n",
        "except:\n",
        "    en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "        train_en.batch(1000).prefetch(2),\n",
        "        **bert_vocab_args\n",
        "    )\n",
        "    write_vocab_file(os.path.join(VOCAB_FILE_DIR, 'en_vocab.txt'), en_vocab)\n",
        "\n",
        "try:\n",
        "  with open(os.path.join(VOCAB_FILE_DIR, 'ru_vocab.txt'), 'r'):\n",
        "    pass\n",
        "except:\n",
        "    ru_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "        train_ru.batch(1000).prefetch(2),\n",
        "        **bert_vocab_args\n",
        "    )\n",
        "    write_vocab_file(os.path.join(VOCAB_FILE_DIR, 'ru_vocab.txt'), ru_vocab)\n"
      ],
      "metadata": {
        "id": "1XlAtNoAMyhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:   \n",
        "\n",
        "    # Include a tokenize signature for a batch of strings. \n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)\n",
        "\n",
        "tokenizers = tf.Module()\n",
        "tokenizers.ru = CustomTokenizer(reserved_tokens, os.path.join(VOCAB_FILE_DIR, 'ru_vocab.txt'))\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, os.path.join(VOCAB_FILE_DIR, 'en_vocab.txt'))"
      ],
      "metadata": {
        "id": "1YcHbZ_1HVeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_pairs(ru, en):\n",
        "  ru = tokenizers.ru.tokenize(ru)\n",
        "  ru = ru.to_tensor()\n",
        "\n",
        "  en = tokenizers.en.tokenize(en)\n",
        "  en = en.to_tensor()\n",
        "  return ru, en\n",
        "\n",
        "  \n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "    ds\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "  )"
      ],
      "metadata": {
        "id": "-39Mu3YTQQiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = make_batches(train_examples)"
      ],
      "metadata": {
        "id": "4ny2tA5LQ1Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(tokenizers, os.path.join(os.getcwd(), 'machine-translation'))"
      ],
      "metadata": {
        "id": "ddqibOviRRJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL"
      ],
      "metadata": {
        "id": "0jyFBjstRnlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POSITIONAL ENCODING"
      ],
      "metadata": {
        "id": "65h4oQidRtSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  i = np.arange(d_model)[np.newaxis,:]\n",
        "  pos = np.arange(position)[:, np.newaxis]\n",
        "\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "  angle_rads = pos * angle_rates\n",
        "\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "ilqXLzKsR4IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MULTIHEAD ATTENTION"
      ],
      "metadata": {
        "id": "lhcaCkGFR5Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    # get dimensions of the input, cast from tensor to float\n",
        "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    \n",
        "    # compute queries x keys and scale by dimension\n",
        "    attention_logits = tf.matmul(q, k, transpose_b=True)\n",
        "    \n",
        "    scaled_attention_logits = attention_logits / tf.math.sqrt(d_k)\n",
        "\n",
        "    # apply decoder mask\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # normalize all scores\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # times value\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.depth = self.d_model // num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "  \n",
        "  def split_heads(self, x, batch_size):\n",
        "    new_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(new_x, perm=[0, 2, 1, 3])\n",
        "  \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.split_heads(self.wq(q), batch_size)\n",
        "    k = self.split_heads(self.wk(k), batch_size)\n",
        "    v = self.split_heads(self.wv(v), batch_size)\n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "    return self.dense(concat_attention), attention_weights"
      ],
      "metadata": {
        "id": "73gZOmnlR__h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ENCODER & DECODER"
      ],
      "metadata": {
        "id": "Kcp5Bs-9SRy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ffnn(d_model: int, d_ff: int):\n",
        "  return tf.keras.Sequential([tf.keras.layers.Dense(d_ff, activation='relu'), tf.keras.layers.Dense(d_model)])"
      ],
      "metadata": {
        "id": "qziDy56XSV1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model: int, num_heads: int, d_ffnn: int, dropout_rate = 0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffnn = ffnn(d_model, d_ffnn)\n",
        "\n",
        "    self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout_1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout_2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    att_output, _ = self.multi_head_attention(x, x, x, mask)\n",
        "    dropout_1 = self.dropout_1(att_output, training=training)\n",
        "    output_1 = self.layer_norm_1(x + dropout_1)\n",
        "\n",
        "    ffnn_output = self.ffnn(output_1)\n",
        "    dropout_2 = self.dropout_2(ffnn_output, training=training)\n",
        "    output_2 = self.layer_norm_2(output_1 + dropout_2)\n",
        "\n",
        "    return output_2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ffnn: int, input_vocab_size: int, maximum_position_encoding, dropout_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, d_ffnn, dropout_rate) for i in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "  \n",
        "  def call(self, x, training, mask):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "        \n",
        "    new_x = self.embedding(x)\n",
        "    new_x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    new_x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    new_x = self.dropout(new_x, training=training)\n",
        "    for i in range(self.num_layers):\n",
        "      new_x = self.enc_layers[i](new_x, training, mask)\n",
        "\n",
        "    return new_x"
      ],
      "metadata": {
        "id": "7g2rQrwmSjrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model: int, num_heads: int, d_ffnn: int, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mmha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffnn = ffnn(d_model, d_ffnn)\n",
        "\n",
        "    self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout_1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout_2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout_3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x, encoder_out, training, look_ahead_mask, padding_mask):\n",
        "    mask_attn_out, attn_weights_block1 = self.mmha(x, x, x, look_ahead_mask)\n",
        "    mask_attn_out = self.dropout_1(mask_attn_out, training=training)\n",
        "    out1 = self.layer_norm_1(mask_attn_out + x)\n",
        "\n",
        "    attn_out, attn_weights_block2 = self.mha(encoder_out, encoder_out, out1, padding_mask)\n",
        "    attn_out = self.dropout_2(attn_out, training=training)\n",
        "    out2 = self.layer_norm_2(attn_out + out1)\n",
        "\n",
        "    ff_out = self.ffnn(out2)\n",
        "    ff_out = self.dropout_3(ff_out, training=training)\n",
        "    out3 = self.layer_norm_3(ff_out + out2)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, target_vocab_size: int, maximum_position_encoding, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, d_ff, dropout_rate) for i in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,  look_ahead_mask, padding_mask):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attn_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)\n",
        "    x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "        x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask,padding_mask)\n",
        "\n",
        "        attn_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "        attn_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    return x, attn_weights"
      ],
      "metadata": {
        "id": "3flr4gYWSn4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HYPERPARAMETER"
      ],
      "metadata": {
        "id": "BuJfoewITvQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "EPOCHS = 20\n",
        "step = 0"
      ],
      "metadata": {
        "id": "hdyEE-VXTxlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TRANSFORMER"
      ],
      "metadata": {
        "id": "HqU79ZOVTQWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, input_vocab_size: int, target_vocab_size: int, pe_input, pe_target, dropout_rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    \n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, input_vocab_size, pe_input, dropout_rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, input_vocab_size, pe_target, dropout_rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "  def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "    dec_output, attention_weights  = self.decoder(tar, enc_output, training, look_ahead_mask,dec_padding_mask)\n",
        "\n",
        "    return self.final_layer(dec_output), attention_weights\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers, \n",
        "    d_model=d_model, \n",
        "    num_heads=num_heads, \n",
        "    d_ff=dff, \n",
        "    input_vocab_size=tokenizers.ru.get_vocab_size(), \n",
        "    target_vocab_size=tokenizers.en.get_vocab_size(), \n",
        "    pe_input=10000, \n",
        "    pe_target=6000,\n",
        "    dropout_rate=dropout_rate,\n",
        ")"
      ],
      "metadata": {
        "id": "Hl9UIjynTSWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING"
      ],
      "metadata": {
        "id": "v_5syQrfTsT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OPTIMIZER"
      ],
      "metadata": {
        "id": "BfgT1FMGU2aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model: int, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "  \n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "metadata": {
        "id": "IwXMQiFrTp3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LOSSES"
      ],
      "metadata": {
        "id": "klqrMxH9VGVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(label, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(label, 0))\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(label, pred):\n",
        "  accuracies = tf.equal(label, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(label, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "IMwfJdRHVIGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PREPARATIONS"
      ],
      "metadata": {
        "id": "IGEg1tMIU7uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size: int):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "LooCi2UjV4AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CHECKPOINTS"
      ],
      "metadata": {
        "id": "A0oWvOgHV2-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./machine-translation/checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "Ax1FndAiXXVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  \n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "StQUkL8BXiKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TRAINING"
      ],
      "metadata": {
        "id": "12RJVA-0X9eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> russian, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 300 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  \n",
        "  print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "metadata": {
        "id": "oEynPZX-X-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVALUATION"
      ],
      "metadata": {
        "id": "3aBq2hUgZF0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, max_length=40):\n",
        "  # inp sentence is russian, hence adding the start and end token\n",
        "  sentence = tf.convert_to_tensor([sentence])\n",
        "  sentence = tokenizers.ru.tokenize(sentence).to_tensor()\n",
        "\n",
        "  encoder_input = sentence\n",
        "\n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  start, end = tokenizers.en.tokenize([''])[0]\n",
        "  output = tf.convert_to_tensor([start])\n",
        "  output = tf.expand_dims(output, 0)\n",
        "\n",
        "  for i in range(max_length):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                output,\n",
        "                                                False,\n",
        "                                                enc_padding_mask,\n",
        "                                                combined_mask,\n",
        "                                                dec_padding_mask)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == end:\n",
        "      break\n",
        "\n",
        "  # output.shape (1, tokens)\n",
        "  text = tokenizers.en.detokenize(output)[0]  # shape: ()\n",
        "\n",
        "  tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "  return text.numpy().decode(\"utf-8\")\n"
      ],
      "metadata": {
        "id": "QCAQw2TbZHdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOXIC CLASSIFICATION"
      ],
      "metadata": {
        "id": "y7fUqSEhIlf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(os.getcwd(), 'toxic_classification', 'train.csv')).fillna(' ')"
      ],
      "metadata": {
        "id": "Y9iFGiZB7D3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['comment_text'] = train_df['comment_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "yeptEyza8rAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000 \n",
        "max_text_length = 100\n",
        "\n",
        "x = train_df['comment_text'].values\n",
        "y = train_df['toxic'].values\n",
        "x_tokenizer = tf.keras.preprocessing.text.Tokenizer(max_features)\n",
        "# Create vocabulary index based on word frequency\n",
        "x_tokenizer.fit_on_texts(list(x))\n",
        "# Transform each text to a sequence of integers mapped to its index in vocabulary index\n",
        "x_tokenized = x_tokenizer.texts_to_sequences(x)\n",
        "# Standardize output shape, longer sequence will be truncated and any shorter will be 0-padded\n",
        "x_train_val = tf.keras.preprocessing.sequence.pad_sequences(x_tokenized, maxlen = max_text_length)"
      ],
      "metadata": {
        "id": "ml3RDfzD_Yyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-trained GloVe (Global Vector) Embeddings "
      ],
      "metadata": {
        "id": "GxoU9cJrBj0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100 # maximum length for the sequences\n",
        "embeddings_index = dict()\n",
        "\n",
        "# Create word embedding matrix for each word in the vocabulary index\n",
        "f = open(os.path.join(os.getcwd(), 'toxic_classification', 'glove', 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "Yt5fDbgEBfDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, index in x_tokenizer.word_index.items():\n",
        "    if index > max_features -1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector"
      ],
      "metadata": {
        "id": "jSC2YBSzBrFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EMBEDDING LAYER"
      ],
      "metadata": {
        "id": "oEkJAjNgB0ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features, embedding_dim,\n",
        "                    embeddings_initializer=tf.keras.initializers.Constant(\n",
        "                    embedding_matrix), trainable=False))\n",
        "model.add(tf.keras.layers.Dropout(0.2))"
      ],
      "metadata": {
        "id": "Vkgfrb39B5_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL"
      ],
      "metadata": {
        "id": "Loh03FatCEWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "filters = 150\n",
        "kernel_size = 3\n",
        "hidden_dims = 150"
      ],
      "metadata": {
        "id": "86D9dErtCFd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling1D())\n",
        "model.add(tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(hidden_dims, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "5vAsMefxCKZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yWuP0zwdChDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING"
      ],
      "metadata": {
        "id": "Ld_sU2sIClDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.30)\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train, batch_size = batch_size,\n",
        "          epochs = epochs, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "-CC7PLARCmO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVALUATION"
      ],
      "metadata": {
        "id": "EETbONJFCzIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(os.path.join(os.getcwd(), 'toxic_classification', 'test.csv'))\n",
        "test_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "IblL7SFDC0wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_test_predictions(df: pd.DataFrame):\n",
        "    df.comment_text = df.comment_text.apply(clean_text)\n",
        "    x_test = df['comment_text'].values\n",
        "    x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
        "    x_testing = tf.keras.preprocessing.sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)\n",
        "    y_testing = model.predict(x_testing, verbose=1, batch_size=64)\n",
        "    df['is_toxic'] = ['not toxic' if x[0] < 0.5 else 'toxic' for x in y_testing]\n",
        "\n",
        "make_test_predictions(test_df)\n",
        "test_df"
      ],
      "metadata": {
        "id": "hEk5a9JNDBak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE INTEGRATION"
      ],
      "metadata": {
        "id": "gGF2qo8524sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LANGUAGE_DETECTION"
      ],
      "metadata": {
        "id": "CrWsyDo28yLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ld_inputs = language_detection_dataset[Split.TEST]['Text'].to_numpy().tolist()[:5]\n",
        "sentences_language = LanguageDetection.evaluate(ld_inputs)\n",
        "sentences_language"
      ],
      "metadata": {
        "id": "9EvgqiRT20k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MACHINE TRANSLATION"
      ],
      "metadata": {
        "id": "xJ7xF1Iv81g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(series: pd.Series):\n",
        "  return evaluate(series[0]) if series[1] == 'ru' else series[0]\n",
        "\n",
        "sentences_language['comment_text'] = sentences_language.apply(test, axis=1)\n",
        "sentences_language"
      ],
      "metadata": {
        "id": "20shz6DZ3ePx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOXIC CLASSIFICATION"
      ],
      "metadata": {
        "id": "hx-xEQ6d80oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_test_predictions(sentences_language)\n",
        "sentences_language"
      ],
      "metadata": {
        "id": "7WlVoynX85wl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}